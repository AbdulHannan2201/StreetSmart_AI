{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab4641-7996-4fc3-a2ad-ddac02a4d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import torchvision.transforms.functional as F\n",
    "import timm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e91d7-76c5-4d67-883a-695734b18aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = '/home/hannan/machineLearning/Dataset/BDD/dataset/dataset/augmented-512x512/train-augment-dataset/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb9111-6d5a-4c7f-8720-7725ed528c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = \"vit_base_patch16_224\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eed1f5-8af7-4378-ab4e-f0e2ea47d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = 7\n",
    " \n",
    "# Batch size\n",
    "batch_size = 16\n",
    " \n",
    "# Number of epochs\n",
    "num_epochs = 200\n",
    " \n",
    "# Flag for feature extracting\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3898ab-a79c-4c39-a02e-f17a9064dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f689844-bdcc-4a86-afde-1053c53df9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87e1de-8d5c-4da9-9daf-1029475b4684",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af533d5e-6611-452a-97f7-a73e70ffb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    " \n",
    "    if model_name == \"vit_base_patch16_224\":\n",
    "        model_ft = timm.create_model('vit_base_patch16_224', pretrained=use_pretrained, num_classes=num_classes)\n",
    "        input_size = 224\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    " \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02023c71-3e94-42bb-b34f-265fa5d3f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa63bb-ae1c-4c68-8153-50dfebaf9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter requires grad\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b871b3-2d04-4c61-9003-83f30a75dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(model_ft, feature_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf8bfe-ebc9-48c0-836f-08d4b3b3c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the parameters to be optimized/updated in this run\n",
    "params_to_update = model_ft.parameters()\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6437630-febb-49ef-902c-f5210193c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "# Setup the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0c584-32b8-4caf-80d0-45a0ccf7eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    checkpoint_path = \"checkpoint.pt\"\n",
    "\n",
    "    # -------- LOAD CHECKPOINT IF EXISTS --------\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"ðŸ”„ Found checkpoint! Loading...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        best_acc = checkpoint[\"best_acc\"]\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "        print(f\"âž¡ Resuming from epoch {start_epoch}, Best Acc = {best_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"âš  No checkpoint found. Starting fresh.\")\n",
    "\n",
    "\n",
    "    # --------------- TRAIN LOOP ---------------\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, \"best_model.pt\")\n",
    "                print(\"ðŸ† New best model saved!\")\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "\n",
    "        # -------- SAVE CHECKPOINT EVERY EPOCH --------\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"best_acc\": best_acc\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        print(f\"ðŸ’¾ Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc_history, val_acc_history, train_loss_history, val_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60365b04-ff2d-4530-9901-ba0d16f59729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and evaluate\n",
    "model_ft, train_acc_hist, val_acc_hist, train_loss_hist, val_loss_hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5335c-3408-482c-bfcd-004cf7aba724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model weights\n",
    "torch.save(model_ft.state_dict(), 'nn-vt-org-weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fb9a6-2a2d-4dac-803d-9acc4d0391ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('nn-vt-org-weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52f9577e-8d83-4326-a702-38fcc226c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_hist = torch.tensor(train_acc_hist)\n",
    "val_acc_hist = torch.tensor(val_acc_hist)\n",
    "train_loss_hist = torch.tensor(train_loss_hist)\n",
    "val_loss_hist = torch.tensor(val_loss_hist)\n",
    "\n",
    "train_acc_hist = train_acc_hist.cpu()\n",
    "val_acc_hist = val_acc_hist.cpu()\n",
    "train_loss_hist = train_loss_hist.cpu()\n",
    "val_loss_hist = val_loss_hist.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "552af808-d7d8-4ecb-9d7a-bc76b3cd471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy and loss\n",
    "def plot_training_history(train_acc_hist, val_acc_hist, train_loss_hist, val_loss_hist, num_epochs):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    " \n",
    "    plt.figure(figsize=(14, 5))\n",
    " \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_acc_hist, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc_hist, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    " \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_loss_hist, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_hist, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a794f4af-9068-48a3-b401-a6f91785fcd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (200,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Call the plot function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_training_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_acc_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_acc_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loss_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mplot_training_history\u001b[39m\u001b[34m(train_acc_hist, val_acc_hist, train_loss_hist, val_loss_hist, num_epochs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Plot accuracy\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_acc_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTraining Accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m plt.plot(epochs, val_acc_hist, label=\u001b[33m'\u001b[39m\u001b[33mValidation Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mTraining and Validation Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/DefectDetection/lib/python3.13/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/DefectDetection/lib/python3.13/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/DefectDetection/lib/python3.13/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/DefectDetection/lib/python3.13/site-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (200,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGyCAYAAAAlL4Q+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHR5JREFUeJzt3X9s3VX9+PFX29FbiLRM59ptFico8nvDjdWChGAqTSDD/WGsYLa58EN0ElyjsjFYRXSdfIAsgcLCBPEPcFMCxLiliJWFADWL25qgbBAcuGls2VTaWbRl7fv7h1+qdd3YLW13bB+P5P6xwzn3nsth3Gfe9/a2IMuyLAAAElV4rDcAAHAkYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBIWt6x8uyzz8b8+fNj+vTpUVBQEE8++eS7rtmyZUt84hOfiFwuFx/96Efj4YcfHsZWAYCJKO9Y6e7ujlmzZkVTU9NRzX/ttdfi8ssvj0suuSTa2tri61//elxzzTXx1FNP5b1ZAGDiKXgvv8iwoKAgnnjiiViwYMFh59x0002xadOm+O1vfzsw9oUvfCHefPPNaG5uHu5DAwATxKTRfoDW1taoqakZNFZbWxtf//rXD7ump6cnenp6Bv7c398ff/3rX+MDH/hAFBQUjNZWAYD3KMuyOHDgQEyfPj0KC0fmo7GjHivt7e1RXl4+aKy8vDy6urriH//4Rxx//PGHrGlsbIzbbrtttLcGAIySvXv3xoc+9KERua9Rj5XhWLFiRdTX1w/8ubOzM04++eTYu3dvlJaWHsOdAQBH0tXVFZWVlXHiiSeO2H2OeqxUVFRER0fHoLGOjo4oLS0d8qpKREQul4tcLnfIeGlpqVgBgP8BI/mxjVH/npXq6upoaWkZNPb0009HdXX1aD80ADAO5B0rf//736OtrS3a2toi4l8/mtzW1hZ79uyJiH+9hbNo0aKB+ddff33s3r07vvWtb8WuXbvivvvui5/85CexbNmykXkGAMC4lnes/OY3v4nzzjsvzjvvvIiIqK+vj/POOy9WrVoVERF//vOfB8IlIuIjH/lIbNq0KZ5++umYNWtW3HXXXfGDH/wgamtrR+gpAADj2Xv6npWx0tXVFWVlZdHZ2ekzKwCQsNF4zfa7gQCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgacOKlaamppg5c2aUlJREVVVVbN269Yjz165dGx//+Mfj+OOPj8rKyli2bFn885//HNaGAYCJJe9Y2bhxY9TX10dDQ0Ns3749Zs2aFbW1tfHGG28MOf/RRx+N5cuXR0NDQ+zcuTMefPDB2LhxY9x8883vefMAwPiXd6zcfffdce2118aSJUvizDPPjHXr1sUJJ5wQDz300JDzX3jhhbjwwgvjqquuipkzZ8all14aV1555btejQEAiMgzVnp7e2Pbtm1RU1Pz7zsoLIyamppobW0dcs0FF1wQ27ZtG4iT3bt3x+bNm+Oyyy477OP09PREV1fXoBsAMDFNymfy/v37o6+vL8rLyweNl5eXx65du4Zcc9VVV8X+/fvjU5/6VGRZFgcPHozrr7/+iG8DNTY2xm233ZbP1gCAcWrUfxpoy5YtsXr16rjvvvti+/bt8fjjj8emTZvi9ttvP+yaFStWRGdn58Bt7969o71NACBReV1ZmTJlShQVFUVHR8eg8Y6OjqioqBhyza233hoLFy6Ma665JiIizjnnnOju7o7rrrsuVq5cGYWFh/ZSLpeLXC6Xz9YAgHEqrysrxcXFMWfOnGhpaRkY6+/vj5aWlqiurh5yzVtvvXVIkBQVFUVERJZl+e4XAJhg8rqyEhFRX18fixcvjrlz58a8efNi7dq10d3dHUuWLImIiEWLFsWMGTOisbExIiLmz58fd999d5x33nlRVVUVr776atx6660xf/78gWgBADicvGOlrq4u9u3bF6tWrYr29vaYPXt2NDc3D3zods+ePYOupNxyyy1RUFAQt9xyS/zpT3+KD37wgzF//vz43ve+N3LPAgAYtwqy/4H3Yrq6uqKsrCw6OzujtLT0WG8HADiM0XjN9ruBAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpw4qVpqammDlzZpSUlERVVVVs3br1iPPffPPNWLp0aUybNi1yuVycdtppsXnz5mFtGACYWCblu2Djxo1RX18f69ati6qqqli7dm3U1tbGyy+/HFOnTj1kfm9vb3zmM5+JqVOnxmOPPRYzZsyIP/zhD3HSSSeNxP4BgHGuIMuyLJ8FVVVVcf7558e9994bERH9/f1RWVkZN9xwQyxfvvyQ+evWrYv/+7//i127dsVxxx03rE12dXVFWVlZdHZ2Rmlp6bDuAwAYfaPxmp3X20C9vb2xbdu2qKmp+fcdFBZGTU1NtLa2DrnmZz/7WVRXV8fSpUujvLw8zj777Fi9enX09fUd9nF6enqiq6tr0A0AmJjyipX9+/dHX19flJeXDxovLy+P9vb2Idfs3r07Hnvssejr64vNmzfHrbfeGnfddVd897vfPezjNDY2RllZ2cCtsrIyn20CAOPIqP80UH9/f0ydOjUeeOCBmDNnTtTV1cXKlStj3bp1h12zYsWK6OzsHLjt3bt3tLcJACQqrw/YTpkyJYqKiqKjo2PQeEdHR1RUVAy5Ztq0aXHcccdFUVHRwNgZZ5wR7e3t0dvbG8XFxYesyeVykcvl8tkaADBO5XVlpbi4OObMmRMtLS0DY/39/dHS0hLV1dVDrrnwwgvj1Vdfjf7+/oGxV155JaZNmzZkqAAA/Ke83waqr6+P9evXx49+9KPYuXNnfOUrX4nu7u5YsmRJREQsWrQoVqxYMTD/K1/5Svz1r3+NG2+8MV555ZXYtGlTrF69OpYuXTpyzwIAGLfy/p6Vurq62LdvX6xatSra29tj9uzZ0dzcPPCh2z179kRh4b8bqLKyMp566qlYtmxZnHvuuTFjxoy48cYb46abbhq5ZwEAjFt5f8/KseB7VgDgf8Mx/54VAICxJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkDStWmpqaYubMmVFSUhJVVVWxdevWo1q3YcOGKCgoiAULFgznYQGACSjvWNm4cWPU19dHQ0NDbN++PWbNmhW1tbXxxhtvHHHd66+/Ht/4xjfioosuGvZmAYCJJ+9Yufvuu+Paa6+NJUuWxJlnnhnr1q2LE044IR566KHDrunr64svfvGLcdttt8Upp5zynjYMAEwsecVKb29vbNu2LWpqav59B4WFUVNTE62trYdd953vfCemTp0aV1999VE9Tk9PT3R1dQ26AQATU16xsn///ujr64vy8vJB4+Xl5dHe3j7kmueeey4efPDBWL9+/VE/TmNjY5SVlQ3cKisr89kmADCOjOpPAx04cCAWLlwY69evjylTphz1uhUrVkRnZ+fAbe/evaO4SwAgZZPymTxlypQoKiqKjo6OQeMdHR1RUVFxyPzf//738frrr8f8+fMHxvr7+//1wJMmxcsvvxynnnrqIetyuVzkcrl8tgYAjFN5XVkpLi6OOXPmREtLy8BYf39/tLS0RHV19SHzTz/99HjxxRejra1t4HbFFVfEJZdcEm1tbd7eAQDeVV5XViIi6uvrY/HixTF37tyYN29erF27Nrq7u2PJkiUREbFo0aKYMWNGNDY2RklJSZx99tmD1p900kkREYeMAwAMJe9Yqauri3379sWqVauivb09Zs+eHc3NzQMfut2zZ08UFvpiXABgZBRkWZYd6028m66urigrK4vOzs4oLS091tsBAA5jNF6zXQIBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKQNK1aamppi5syZUVJSElVVVbF169bDzl2/fn1cdNFFMXny5Jg8eXLU1NQccT4AwH/KO1Y2btwY9fX10dDQENu3b49Zs2ZFbW1tvPHGG0PO37JlS1x55ZXxzDPPRGtra1RWVsall14af/rTn97z5gGA8a8gy7IsnwVVVVVx/vnnx7333hsREf39/VFZWRk33HBDLF++/F3X9/X1xeTJk+Pee++NRYsWHdVjdnV1RVlZWXR2dkZpaWk+2wUAxtBovGbndWWlt7c3tm3bFjU1Nf++g8LCqKmpidbW1qO6j7feeivefvvteP/733/YOT09PdHV1TXoBgBMTHnFyv79+6Ovry/Ky8sHjZeXl0d7e/tR3cdNN90U06dPHxQ8/62xsTHKysoGbpWVlflsEwAYR8b0p4HWrFkTGzZsiCeeeCJKSkoOO2/FihXR2dk5cNu7d+8Y7hIASMmkfCZPmTIlioqKoqOjY9B4R0dHVFRUHHHtnXfeGWvWrIlf/vKXce655x5xbi6Xi1wul8/WAIBxKq8rK8XFxTFnzpxoaWkZGOvv74+Wlpaorq4+7Lo77rgjbr/99mhubo65c+cOf7cAwIST15WViIj6+vpYvHhxzJ07N+bNmxdr166N7u7uWLJkSURELFq0KGbMmBGNjY0REfH9738/Vq1aFY8++mjMnDlz4LMt73vf++J973vfCD4VAGA8yjtW6urqYt++fbFq1apob2+P2bNnR3Nz88CHbvfs2ROFhf++YHP//fdHb29vfO5znxt0Pw0NDfHtb3/7ve0eABj38v6elWPB96wAwP+GY/49KwAAY02sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRtWrDQ1NcXMmTOjpKQkqqqqYuvWrUec/9Of/jROP/30KCkpiXPOOSc2b948rM0CABNP3rGycePGqK+vj4aGhti+fXvMmjUramtr44033hhy/gsvvBBXXnllXH311bFjx45YsGBBLFiwIH7729++580DAONfQZZlWT4Lqqqq4vzzz4977703IiL6+/ujsrIybrjhhli+fPkh8+vq6qK7uzt+/vOfD4x98pOfjNmzZ8e6deuO6jG7urqirKwsOjs7o7S0NJ/tAgBjaDResyflM7m3tze2bdsWK1asGBgrLCyMmpqaaG1tHXJNa2tr1NfXDxqrra2NJ5988rCP09PTEz09PQN/7uzsjIh//QsAANL1zmt1ntdCjiivWNm/f3/09fVFeXn5oPHy8vLYtWvXkGva29uHnN/e3n7Yx2lsbIzbbrvtkPHKysp8tgsAHCN/+ctfoqysbETuK69YGSsrVqwYdDXmzTffjA9/+MOxZ8+eEXvi5K+rqysqKytj79693o47xpxFOpxFGpxDOjo7O+Pkk0+O97///SN2n3nFypQpU6KoqCg6OjoGjXd0dERFRcWQayoqKvKaHxGRy+Uil8sdMl5WVuY/wgSUlpY6h0Q4i3Q4izQ4h3QUFo7ct6PkdU/FxcUxZ86caGlpGRjr7++PlpaWqK6uHnJNdXX1oPkREU8//fRh5wMA/Ke83waqr6+PxYsXx9y5c2PevHmxdu3a6O7ujiVLlkRExKJFi2LGjBnR2NgYERE33nhjXHzxxXHXXXfF5ZdfHhs2bIjf/OY38cADD4zsMwEAxqW8Y6Wuri727dsXq1ativb29pg9e3Y0NzcPfIh2z549gy79XHDBBfHoo4/GLbfcEjfffHN87GMfiyeffDLOPvvso37MXC4XDQ0NQ741xNhxDulwFulwFmlwDukYjbPI+3tWAADGkt8NBAAkTawAAEkTKwBA0sQKAJC0ZGKlqakpZs6cGSUlJVFVVRVbt2494vyf/vSncfrpp0dJSUmcc845sXnz5jHa6fiWzzmsX78+Lrroopg8eXJMnjw5ampq3vXcOHr5/p14x4YNG6KgoCAWLFgwuhucQPI9izfffDOWLl0a06ZNi1wuF6eddpr/R42AfM9h7dq18fGPfzyOP/74qKysjGXLlsU///nPMdrt+PTss8/G/PnzY/r06VFQUHDE3/P3ji1btsQnPvGJyOVy8dGPfjQefvjh/B84S8CGDRuy4uLi7KGHHsp+97vfZddee2120kknZR0dHUPOf/7557OioqLsjjvuyF566aXslltuyY477rjsxRdfHOOdjy/5nsNVV12VNTU1ZTt27Mh27tyZfelLX8rKysqyP/7xj2O88/En37N4x2uvvZbNmDEju+iii7LPfvazY7PZcS7fs+jp6cnmzp2bXXbZZdlzzz2Xvfbaa9mWLVuytra2Md75+JLvOTzyyCNZLpfLHnnkkey1117LnnrqqWzatGnZsmXLxnjn48vmzZuzlStXZo8//ngWEdkTTzxxxPm7d+/OTjjhhKy+vj576aWXsnvuuScrKirKmpub83rcJGJl3rx52dKlSwf+3NfXl02fPj1rbGwccv7nP//57PLLLx80VlVVlX35y18e1X2Od/mew387ePBgduKJJ2Y/+tGPRmuLE8ZwzuLgwYPZBRdckP3gBz/IFi9eLFZGSL5ncf/992ennHJK1tvbO1ZbnBDyPYelS5dmn/70pweN1dfXZxdeeOGo7nMiOZpY+da3vpWdddZZg8bq6uqy2travB7rmL8N1NvbG9u2bYuampqBscLCwqipqYnW1tYh17S2tg6aHxFRW1t72Pm8u+Gcw39766234u233x7RX141EQ33LL7zne/E1KlT4+qrrx6LbU4IwzmLn/3sZ1FdXR1Lly6N8vLyOPvss2P16tXR19c3Vtsed4ZzDhdccEFs27Zt4K2i3bt3x+bNm+Oyyy4bkz3zLyP1en3Mf+vy/v37o6+vb+AbcN9RXl4eu3btGnJNe3v7kPPb29tHbZ/j3XDO4b/ddNNNMX369EP+wyQ/wzmL5557Lh588MFoa2sbgx1OHMM5i927d8evfvWr+OIXvxibN2+OV199Nb761a/G22+/HQ0NDWOx7XFnOOdw1VVXxf79++NTn/pUZFkWBw8ejOuvvz5uvvnmsdgy/9/hXq+7urriH//4Rxx//PFHdT/H/MoK48OaNWtiw4YN8cQTT0RJScmx3s6EcuDAgVi4cGGsX78+pkyZcqy3M+H19/fH1KlT44EHHog5c+ZEXV1drFy5MtatW3estzahbNmyJVavXh333XdfbN++PR5//PHYtGlT3H777cd6awzDMb+yMmXKlCgqKoqOjo5B4x0dHVFRUTHkmoqKirzm8+6Gcw7vuPPOO2PNmjXxy1/+Ms4999zR3OaEkO9Z/P73v4/XX3895s+fPzDW398fERGTJk2Kl19+OU499dTR3fQ4NZy/F9OmTYvjjjsuioqKBsbOOOOMaG9vj97e3iguLh7VPY9HwzmHW2+9NRYuXBjXXHNNREScc8450d3dHdddd12sXLly0O+wY/Qc7vW6tLT0qK+qRCRwZaW4uDjmzJkTLS0tA2P9/f3R0tIS1dXVQ66prq4eND8i4umnnz7sfN7dcM4hIuKOO+6I22+/PZqbm2Pu3LljsdVxL9+zOP300+PFF1+Mtra2gdsVV1wRl1xySbS1tUVlZeVYbn9cGc7fiwsvvDBeffXVgWCMiHjllVdi2rRpQmWYhnMOb7311iFB8k5AZn4l3pgZsdfr/D77Ozo2bNiQ5XK57OGHH85eeuml7LrrrstOOumkrL29PcuyLFu4cGG2fPnygfnPP/98NmnSpOzOO+/Mdu7cmTU0NPjR5RGQ7zmsWbMmKy4uzh577LHsz3/+88DtwIEDx+opjBv5nsV/89NAIyffs9izZ0924oknZl/72teyl19+Ofv5z3+eTZ06Nfvud797rJ7CuJDvOTQ0NGQnnnhi9uMf/zjbvXt39otf/CI79dRTs89//vPH6imMCwcOHMh27NiR7dixI4uI7O6778527NiR/eEPf8iyLMuWL1+eLVy4cGD+Oz+6/M1vfjPbuXNn1tTU9L/7o8tZlmX33HNPdvLJJ2fFxcXZvHnzsl//+tcD/+ziiy/OFi9ePGj+T37yk+y0007LiouLs7POOivbtGnTGO94fMrnHD784Q9nEXHIraGhYew3Pg7l+3fiP4mVkZXvWbzwwgtZVVVVlsvlslNOOSX73ve+lx08eHCMdz3+5HMOb7/9dvbtb387O/XUU7OSkpKssrIy++pXv5r97W9/G/uNjyPPPPPMkP/ff+ff/eLFi7OLL774kDWzZ8/OiouLs1NOOSX74Q9/mPfjFmSZ62EAQLqO+WdWAACORKwAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkLT/B2bvhUyp908MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the plot function\n",
    "plot_training_history(train_acc_hist, val_acc_hist, train_loss_hist, val_loss_hist, num_epochs)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ad1f1-bd55-4985-85d3-d1b6e1daae50",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875cc4f-e027-4b76-889c-9da29e1ef580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "model_ft.eval()\n",
    "test_corrects = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    " \n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    " \n",
    "    outputs = model_ft(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    " \n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    test_corrects += torch.sum(preds == labels.data)\n",
    " \n",
    "test_acc = test_corrects.double() / dataset_sizes['test']\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a594964-30f0-4f98-8dfc-88f78ff39c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93b631-98ec-4621-9d0b-f2e1d8cab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix and classification report\n",
    "conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38033dc7-394c-4a6f-a1c4-76fd91f2f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels and predictions to their corresponding class names\n",
    "actual_class_names = [class_names[label] for label in all_labels]\n",
    "predicted_class_names = [class_names[pred] for pred in all_preds]\n",
    "\n",
    "# Create the DataFrame\n",
    "res = pd.DataFrame({\"actual\": actual_class_names, \"predicted\": predicted_class_names})\n",
    "\n",
    "#res = pd.DataFrame( {\"actual\": all_labels, \"predicted\": all_preds})\n",
    "print(res)\n",
    "res.to_csv('cura_vt_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a799b9-ef75-4474-a192-5a8cca3335f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('curat_vt_confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76896c3b-2ced-432a-8b2f-54717158ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(model, dataloaders, class_names, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    " \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    " \n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'Predicted: {class_names[preds[j]]}')\n",
    "                plt.imshow(F.to_pil_image(inputs.cpu().data[j]))\n",
    " \n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    " \n",
    "# Visualize predictions\n",
    "visualize_predictions(model_ft, dataloaders, class_names, num_images=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b007e98-526a-4094-a487-3a5e78793a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display evaluation metrics\n",
    "y_true = np.array(all_labels)\n",
    "y_pred = np.array(all_preds)\n",
    " \n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "precision = np.diag(conf_mat) / np.sum(conf_mat, axis=0)\n",
    "recall = np.diag(conf_mat) / np.sum(conf_mat, axis=1)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    " \n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print the average precision, recall, and F1-score\n",
    "avg_precision = np.mean(precision)\n",
    "avg_recall = np.mean(recall)\n",
    "avg_f1_score = np.mean(f1_score)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-Score: {avg_f1_score:.4f}\")\n",
    " \n",
    "# Ensure the confusion matrix and the classification report are saved\n",
    "with open('curat_vt_classification_report.txt', 'w') as f:\n",
    "    f.write(f\"Classification Report:\\n{class_report}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55253379-165f-41b8-8c4c-42b9952b3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08bf53-d4e7-480e-9ecb-9fd653826c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.load_state_dict(torch.load('nn-vt-org-weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b423e6-2e66-40e2-9056-acbf643d7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb553c-1101-4efd-bf9d-b966b84a068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_ft.eval()\n",
    "model_ft.to(device)\n",
    "# Create the attention folder if it doesn't exist\n",
    "attention_dir = os.path.join(data_dir, 'attention')\n",
    "os.makedirs(attention_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model_ft(inputs)\n",
    "\n",
    "    # Get the attention map\n",
    "    attention_map = model_ft.get_attention_map(inputs)\n",
    "\n",
    "    # Save the attention map\n",
    "    for i, img in enumerate(inputs):\n",
    "        attention_img = F.to_pil_image(attention_map[i])\n",
    "        attention_img.save(os.path.join(attention_dir, f'test_image_{i}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca766e-d01d-4857-9954-8476a77f2756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
